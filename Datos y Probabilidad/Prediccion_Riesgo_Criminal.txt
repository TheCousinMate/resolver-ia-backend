The University of Sydney Law School Legal Studies Research Paper Series No. 19/67 November 2019 Predicting risk in criminal procedure: actuarial tools, algorithms, AI and judicial decision-making Carolyn McKay This paper can be downloaded without charge from the Social Science Research Network Electronic Library at: Predicting risk in criminal procedure: actuarial tools, algorithms, AI and judicial decision-making “This is an original manuscript / preprint of an article published by Taylor & Francis in Current Issues in Criminal Justice on 29 September 2019, available online: Risk assessments are conducted at a number of decision points in criminal procedure including in bail, sentencing and parole as well as in determining extended supervision and continuing detention orders of high risk offenders. Such risk assessments have traditionally been the function of the human discretion and intuition of judicial officers, based on clinical assessments, framed by legislation and common law principles, and encapsulating the concept of individualised justice. Yet the progressive technologisation of criminal procedure is witnessing the incursion of statistical, data-driven evaluations of risk. Human judicial evaluative functions are increasingly complemented by a range of actuarial, algorithmic, machine learning and Artificial Intelligence (AI) tools that purport to provide accurate predictive capabilities, and objective, consistent risk assessments. But ethical concerns have been raised globally regarding algorithms as proprietary products with in-built statistical bias as well as the diminution of judicial human evaluation in favour of the machine. This article focuses on risk assessment and what happens when decision-making is delegated to a predictive tool. Specifically, this article scrutinises the inscrutable proprietary nature of such risk tools and how that may render the calculation of the risk score opaque and unknowable to both the offender and the court. Keywords: artificial intelligence; algorithms; actuarial tools; ethics; risk assessment; criminal procedure Introduction In the criminal jurisdiction, gauging unacceptable risk, high risk, risk to community safety as well as forecasting the likelihood of reoffending and prospects of rehabilitation are necessary judicial tasks in bail, sentencing and parole procedures that determine the liberty of an accused or offender. In addition, risk assessments are made for the extended supervision and continuing detention of high risk sex, terrorism and violent offenders to ensure the safety and protection of the community and to promote their rehabilitation (for example, Crimes (High Risk Offenders) Act 2006 (NSW) s 3(1)). Such risk assessments have traditionally been the function of the human discretion and intuition of judicial officers, based on clinical assessments, framed by legislation and common law principles, and encapsulating the concept of individualised justice (Markarian v The Queen [2005] HCA 25; Muldrock v The Queen [2011] HCA 39; Bugmy v The Queen [2013] HCA 37). Yet the progressive technologisation of criminal procedure is witnessing the incursion of statistical, data-driven evaluations of risk. Human judicial evaluative functions are increasingly complemented by a range of actuarial, algorithmic, machine learning and Artificial Intelligence (AI) tools that purport to provide accurate predictive capabilities, and objective and consistent assessments of risk (Barabas et al. 2017). But ethical and human rights concerns have been raised globally regarding algorithms as proprietary products, potentially with in- built statistical bias as well as the diminution of judicial human evaluation in favour of the machine (Dawson et al. 2019; Angwin et al. 2016; Barabas et al. 2017; European Commission 2019; Wexler 2017). A recent report from England and Wales found there was ‘a lack of explicit standards, best practice, and openness or transparency about the use of algorithmic systems in criminal justice’ (Law Society 2019: 4). It is in this context that the need for ethical and human rights-based AI frameworks in Australia has been recently recognised (Dawson et al. 2019; AHRC 2018). Actuarial models, algorithms and AI frame a growing number of technologies used in criminal justice, for instance, in automated decisions, predictive policing and facial recognition (Law Society 2019). This article will focus on risk assessment and what happens when decision-making is delegated to a predictive tool. Specifically, this article will scrutinise the inscrutable proprietary nature of such risk tools and how the calculation of the risk score may be rendered opaque and unknowable to both the offender and the court. This is a significant area to explore given that the decision points in criminal procedure where algorithmic instruments may be applied, represent the ultimate high stakes determination of liberty versus detention in a situation of extreme power imbalance (Barabas et al. 2017; Eckhouse et al. 2019; Law Society 2019). This article commences with an analysis of risk in the context of an increasingly risk-averse society and criminal justice system, and the tensions between a general right to be at liberty versus community safety. In the next section, the article examines criticisms levelled at human judicial discretion, particularly in the context of sentencing. Flowing from this discussion, the article provides an overview of the development of predictive tools in risk assessments in criminal procedure to question whether machine and data-driven assessments offer more accuracy and objectivity than human judges. Algorithmic instruments and issues concerning embedded bias and proprietary interests are then examined through the lens of the now infamous United States (US) decision of State of Wisconsin v Loomis 881 N.W.2d 749 (Wis. 2016). Finally, the need to revisit the concept of procedural justice is examined in the context of a progressively technologised criminal justice system. The application of statistical, data-based instruments in deterministic criminal procedures may transgress the presumption of innocence in bail applications, breach individualised justice in sentencing and parole, further constrain or displace judicial discretion, and diminish procedural justice. Risk, Unacceptable Risk and Protecting the Community The assessment of risk has become a critical element of the criminal justice system from law enforcement through to pre- and post-trial procedures including bail and sentencing as well as in decisions regarding an offender’s release back into the community or continuing detention or extended supervision (Carlson 2017; Harcourt 2005). Recent case law demonstrates judicial engagement with risk-related terminology including risk management, risk profile, risk factors, risk associated behaviour, and risk of recidivism. Case law also reveals how courts gauge concepts such as unacceptable risk, high risk, and risk to community safety. What is ‘risk’ and why has it become a focal point in criminal justice? Scholars have identified the increasing emphasis on ‘dangerousness, risk, pre- emption and uncertainty’ so that precaution and risk prevention have become dominant forces in the administration of justice, perhaps privileging ‘generalised fears’ and potential harms over ‘real threats’ (Brown et al. 2015: 43-44). In our risk society (Beck 1992), risk is a ‘core organising concept’ (McSherry 2004: 1) and there is a clear political context that frames debates regarding law and order (Brown and Quilter 2014) as well as moral panics (Lee 2007) leading to the increased significance of risk assessment and tightening of legislative, policing and procedural measures. The concept of risk has justified the extension of criminal responsibility to behaviours associated with consorting, planning acts of terrorism, status offences such as being the member of an outlaw motor cycle gang, and has fuelled new forms of post-sentence supervision and preventive detention (Brown et al. 2015). The spread of criminal law’s dominion over possible future harms and inchoate or preparatory criminality may be analysed as ‘risk-neutralization’ or preventive justice on the one hand while on the other, as diminishing the rule of law and presumption of innocence (O’Malley 2013: 276; Zedner 2007; Husak 2008; Brown et al. 2015). Risk assessments are undertaken at various criminal procedure decision points including at bail, sentencing and parole. Pending trial, an accused person may be held on remand and using NSW as an example, courts consider the composite phrase ‘unacceptable risk’, that is, an unacceptable risk that the accused person, if released from custody, will fail to appear at proceedings, commit a serious offence, endanger the safety of victims, individuals or the community, or interfere with witnesses or evidence (Bail Act 2013 (NSW)). Predicting such risk is ‘context specific’ (Lynn v State of New South Wales [2016] NSWCA 57, Beazley P at [74]), complex and controversial given that ‘it is trite to observe that no grant of bail is risk free’ (R v Elzamtar [2017] NSWSC 275, Harrison J at [23]). That is, the bail decision does not rest on the elimination of risk, nor probabilities of the risk, rather: What must be established is that there is a sufficient likelihood of the occurrence of the risk which, having regard to all relevant circumstances, makes it unacceptable. Hence the possibility an offender may commit like offences has been viewed as sufficient to satisfy a court that there is an unacceptable risk. (Haidy v DPP [2004] VSC 247, Redlich J at [16]). Whether a risk is unacceptable is contentious (R v Agang; R v Bajwa; R v Ghanem [2017] NSWSC 138) as noted by Harrison J at [17]: I accept that the existence of a risk and the assessment of whether or not it is unacceptable are matters about which minds may differ…Inevitably and too often one is required to make the determination based only on contested inferences from the past and frail predictions about the future. This observation highlights the complexity of pre-trial ‘frail predictions’, signals possible inconsistencies between judges and may offer an insight as to why statistical risk assessment tools are being adopted throughout many jurisdictions (Koepke and Robinson 2018). Risk assessments and ‘predictions of future criminality’ (ALRC 2005: 51) in sentencing can arise in a number of other circumstances. Certainly, one purpose of sentencing is to protect the community from the offender and incapacitation through incarceration eliminates the risk of re-offending while the offender is detained. Risk is also significant when making an Intensive Correction Order (ICO), a form of custodial sentence served in the community. The paramount consideration in making such an order is community safety and, in this evaluation, the court is to address the offender’s risk of reoffending (see for example Crimes (Sentencing Procedure) Act 1999 (NSW) ss 7, 66). In the general sentencing context, the Veen series of cases provides a compelling examination of the risk of recidivism, public protection, preventive detention as well as indefinite detention (McSherry 2004; Veen (no. 1) (1979) 143 CLR 458; Veen (no. 2) (1988) 164 CLR 465). Risk assessment may be a critical function within the prison system too, for example, in the security classification of inmates, identifying inmates’ risks and criminogenic needs, and in offender management (Law Society 2019; Moore 2015). Risk assessments are also fundamental at the other end of criminal process – when an offender may be eligible to be released from a custodial situation on parole back into the community. In effect, parole means that the offender may serve the remainder of their sentence in the community subject to supervision and strict conditions. In parole determinations, risk to community safety is central as well as risk of reoffending, risks to the offender and risks to other persons (see for example Crimes (Administration of Sentences) Act 1999 (NSW) Part 6; ss 128, 130; Crimes (Administration of Sentences) Regulation 2014 (NSW) Parts 14, 14A). Regarding high risk offenders and the imposition of extended supervision orders (a mode of supervising an offender in the community at the expiration of their custodial sentence) or continuing detention orders (a form of preventative, protective and indeterminate detention beyond the custodial sentence period), courts must consider the composite phrase ‘unacceptable risk’ (see for example Crimes (High Risk Offenders) Act 2006 (NSW)). This is done in the context of ‘making the community secure from harm as opposed to guaranteeing its safety and protection…were it otherwise, every risk would be unacceptable’ (Lynn v State of New South Wales [2016] NSWCA 57, Beazley P at [61] my emphasis). This requires a consideration of firstly, ‘the probability that the risk will manifest’ and secondly, the seriousness of the potential harm (State of NSW v Ceissman [2018] NSWSC 508, Rothman J at [26]). The assessment of the unacceptability of any risk ‘involves at least notionally the arithmetical product of the consequences of the risk should it eventuate on the one hand and the likelihood that it will eventuate on the other hand’ (State of New South Wales v Pacey (Final) [2015] NSWSC 1983 Harrison J at [43]). Given the close affiliation with between calculations of dangerousness (Hobbs and Trotter 2018), risk assessments and arithmetic predictions, the rise of statistical, actuarial or algorithmic evaluation is not surprising. Objecting to the Subjectivity of Judges At the same time as the emergence of an increasingly risk-adverse society, there has been a trend towards criticising the human discretion that operates at all levels of the criminal justice system and is, perhaps, made most public in sentencing judgements. The public and media not infrequently express frustration with sentencing decisions which seem too lenient, unfair or inconsistent with other decisions (Zdenkowski 2000). Scholars, too, critique the levels of unpredictability and numerical inconsistency in sentencing decisions (Stobbs et al. 2017; Krasnostein and Freiberg 2013). However, many decisions made throughout criminal justice are premised upon the concept of individualised justice and the exercise of human discretion: from police officers’ discretion to arrest, to judicial discretion in granting or denying release to bail, to sentencing and to the post-sentence orders that may be made in relation to high risk offenders. The principle of individualised justice is responsive to the individual offender, the facts and the offence (Anthony et al. 2015) and embraces the notion that ‘there is no greater inequality than the equal treatment of unequals’ (Dennis v United States 339 US 162, 184 (Frankfurter J) (1950)). It ensures that the punishment fits the crime and the offender’s moral culpability (R v Fernando (1992) 76 A Crim R 58; Bugmy v The Queen (2013) 249 CLR 571; Munda v Western Australia (2013) 249 CLR 600; Elias v The Queen (2013) 248 CLR 483). This principle is particularly relevant in sentencing where the judiciary recognises that ‘the outcome of discretionary decision- making can never be uniform’ (Wong v The Queen [2001] HCA 54, Gleeson CJ at [6]) and there is no singular ‘correct’ sentence (Martin 2017: 19) as every offender and offence is different (Anthony et al. 2015). There is, however, a proviso that ‘like cases should be treated in a like manner’ (Wong v The Queen [2001] HCA 54, Gleeson CJ at [6]). Of course, the exercise of discretion in criminal justice is not completely unfettered with legislation, legal precedent and guidelines serving to delimit discretion (Martin 2017). For instance in sentencing, judges may be constrained by guideline judgments, mandatory minimum sentences, maximum penalties, the principles that imprisonment ought to be as a last resort, proscribed sentencing ‘discounts’, aggravating and mitigating factors and non-parole periods. Within this process, numerical consistency is less important than the consistent application of legal principles (Hili v The Queen (2010) 242 CLR 520). Ultimately in sentencing, judges are required to assess the appropriate sentence given the particular offence committed by the particular offender, while balancing objective and subjective factors with the purposes of sentencing that includes punishment, deterrence, community protection, rehabilitation, accountability, denouncement and recognition of the harm inflicted on the victim and the community. This is a complex exercise in proportionality and giving weight to all the competing and multiple objectives of sentencing, a process referred to as ‘instinctive’ or ‘intuitive’ synthesis. Instinctive synthesis has been described as a global value judgment, recognised as not necessarily logical and a process that may produce ‘outcomes upon which reasonable minds will differ’ (Hudson v The Queen (2010) 30 VR 610; 205 A Crim R 199; [2010] VSCA 332 at [27]). However well the process of instinctive synthesis is articulated in sentencing judgments, it seems to many to be an opaque, arbitrary, unpredictable and perhaps overly subjective process (Stobbs et al. 2017). Would the procedure be more transparent and fair if instinctive synthesis and related risk assessments by humans were instead solved or structured by an algorithm, an AI judge (Sourdin 2018)? Stobbs et al. (2017: 262) suggest that sentencing is amenable to being automated because it is premised upon established principles, weightings and key factors. AI is well suited to the complexities of calibrating multiple variables, if not presenting a means to improve and refine the sentencing system by incorporating algorithmic risk assessments and by removing the ‘subconscious bias’ of humans. Likewise, vendors of predictive risk assessment tools promote them as the solution to human bias and an improvement on human judgment (Eckhouse et al. 2019). Non-human automated decision-making processes purport to make such assessments more consistent, timely, cost-effective and accurate (Hogan-Doran 2017). The Growth of the Algorithm The push-back against judicial subjectivity and discretion needs to be examined in the context of preventive justice (Ashworth and Zedner 2014) as well as the rise of ‘actuarial justice’ and the embedding of scientific discourse in criminal justice (O’Malley 2013: 276; Brown et al 2015; McSherry 2004). While courts in some jurisdictions have been concerned with questions of offenders’ current and future dangerousness, there have been definite shifts towards risk-based models and ‘statistical or actuarial risk prediction’ (McSherry 2004: 2; Koepke and Robinson 2018). At the outset, key terms need to be defined as there are various statistical, data-driven predictive tools used in criminal procedure risk assessments that are not strictly AI. Rather they are ‘actuarial’ or ‘algorithmic’ instruments. However, similar ethical concerns and challenges arise. Barabas et al. (2017) state that actuarial decision-making practices in risk assessment have been used since the 1920s. According to Harcourt (2005: 10) the term ‘actuarial’ refers to the use of statistical methods—rather than clinical methods—on large datasets of criminal offending rates … to determine the different levels of offending associated with a group … and, based on those correlations, to predict first the past, present or future criminal behavior of a particular individual and to administer second a criminal justice outcome for that particular individual. Actuarial can also refer to the fact that the score is determined by an algorithm (Smid 2014). Algorithms can be understood in various ways such as relating to a defined computational procedure or set of instructions that takes a set of values as input and produces a set of values as output (Law Society 2019). In relation to criminal justice, an algorithm can be understood as a rule that uses numerical inputs to produce a prediction relevant to the procedural decision point (Christin et al. 2015). Algorithms operate in different ways and more advanced forms include machine learning whereby the machine ‘learns’, improves its tasks over time and may modify an algorithm as it synthesises new data (Law Society 2019: 10). Regarding AI, it is suggested that there is ‘no universally accepted definition’; it is an expression that encapsulates autonomous computerised processing of data that resembles or replicates human processing and intelligence (AHRC 2018: 26). AI-informed decision-making and prediction occurs when algorithms are applied to datasets with tasks ranging from simple, narrow automated systems to more sophisticated ‘neural nets and deep learning’ (Dawson et al. 2019: 14). Prior to the uptake of actuarial or algorithmic techniques, risk was assessed in a clinical but human manner, for example, by psychiatrists or psychologists, based on professional, subjective evaluation (Carlson 2017), basically, ‘unstructured clinical judgements’ (Jones and Milton 2016: 1; Hsu et al. 2009; Moore 2015). Now this wholly human approach may be considered as overly subjective and lacking in reliability and consistency (Jones and Milton 2016). The various generations of risk assessment have developed through the actuarial or statistical approach and have aimed for a greater level of objectivity (Hsu et al. 2009). They were initially based on ‘static factors (the need-to-know aspects of the offenders such as age at first offense and crime(s) committed)’, then later combined with ‘dynamic factors (the possibility of change in the offenders’ lives)’ (Hsu et al. 2009: 729), and more recently with other specific offender factors to enable treatment and intervention (Moore 2015). Risk assessment tools, in essence, use data regarding groups of people, a range of factors and weightings, and human-inputted rules to predict an individual’s future behaviour (Koepke and Robinson 2018). Such instruments provide statistical predictions, typically comprising risk factors as predictors of violence or reoffending so that an individual is evaluated against these risk factors and ‘scored’ – the higher the score, the higher the risk (Carlson 2017; Grann and Långström 2007). These predictive tools have permeated law enforcement and the criminal justice system (Harcourt 2005) to become the norm in practice (Smid 2014), and ‘used as an objective, neutral mechanism of fair treatment’ (Eckhouse et al. 2019: 3). The instruments exude the veneer of objectivity and are used to score the risks of ‘flight, rearrest, parole violation…based on data from other people with characteristics similar’ (Eckhouse et al. 2019: 3). In summary, it is clear that subjective risk assessments have increasingly given way to mechanical and actuarial tools that produce statistical models based on extensive criminal offending datasets, perceived to be more objective than human observation (Carlson 2017). Forms of predictive risk assessment are now applied at various decision points in criminal justice and ‘dominate the field of crime and punishment’ such that there is an ‘actuarial turn’ in criminal law (Harcourt 2005: 15). Law enforcement agencies have adopted predictive algorithmic instruments for a number of risk assessment applications. For instance in England and Wales, the Harm Assessment Risk Tool (HART), described as a ‘random forest algorithm’, is deployed in determining whether individuals should be arrested, charged or diverted (Law Society 2019: 46). Suspects are risk-scored using a variety of other algorithmic tools and matrices that seek to identify propensities for particular forms of offending and future behaviours, while predictive systems are also used to identify risks of victimisation (Law Society 2019). In the sentencing context, pre-sentence reports routinely inform courts of the offender’s risk score (Law Society 2019). Predictive Tools in Action Recent NSW case law evidences the range of predictive, diagnostic tools including Level of Service Inventory - Revised (LSI-R), the Risk Assessment Report, the STABLE-2007 tool, the STATIC Risk Factors Actuarial Assessment - Sex Offending (STATIC-99R) and the Risk of Sexual Violence Protocol (RSVP). How can such emergent actuarial/algorithmic tools assist judicial officers in making decisions that impact a defendant’s or offender’s legal status and liberty? Can criminal procedure decisions be partially or fully automated or, at least, structured by an algorithm? In State of New South Wales v Barrie (Final) [2018] NSWSC 1005, the court discussed the risk assessment of a high risk sexual offender in relation to a continuing detention order following the expiry of his custodial sentence. Regarding the likelihood of the offender committing further serious crimes, Adam J at [69] observed that the assessing psychologist had acknowledged that it was ‘not scientifically possible to accurately predict whether or not a specific offender will or will not actually reoffend.’ Indeed the unpredictability of human behaviour means that those who need to assess risk increasingly make use of various approaches (Jones and Milton 2016). In Barrie, the psychologist assessed his ‘risk of sexual reoffending using the STATIC-99R, which is an actuarial test applied in predicting the sexual recidivism for individuals charged with or convicted of sexual offences’ (State of New South Wales v Barrie (Final) [2018] NSWSC 1005, N Adams J at [69]), a tool described in another case as having ‘moderate predictive accuracy’ (State of New South Wales v Graham James Kay [2018] NSWSC 1235, Wilson J at [40]). Using the STATIC-99R tool, the offender in Barrie was assessed as a “7” placing him in the high risk category. The assessing psychologist in this case acknowledged the ‘limitations of this test and stated that a more comprehensive evaluation was obtained by reference to the RSVP (Risk of Sexual Violence Protocol)’ (State of New South Wales v Barrie (Final) [2018] NSWSC 1005, N Adams J at [69]). The RSVP tool is a structured professional judgment instrument developed to assist in the identification and management of sexual violence using a range of factors identified by the literature related to sexual offending. It includes 22 static and dynamic factors grouped into five domains. Those five domains are: a history of sexual violence; psychological adjustment; mental disorder; social adjustment; and manageability. She concluded that the defendant presented with risk factors in all but one of those domains (he [did] not have a mental disorder). She state[d] that this suggests that the STATIC-99R assessment of high risk is an accurate reflection of his risk. (State of New South Wales v Barrie (Final) [2018] NSWSC 1005, N Adams J at [69]). Here the psychologist used the RSVP tool, described as the most common Structured Professional Judgement instrument for high risk sexual offenders, as a means to structure the human risk assessor’s process of evaluation rather than replace that person (Jones and Milton 2016). In State of New South Wales v Graham James Kay [2018] NSWSC 1235, while evidence was provided from three actuarial tools, the inability to scientifically predict whether the offender would reoffend was again noted, given the actuarial instruments were based on historical factors. One actuarial assessment instrument used was the Level of Service Inventory - Revised (LSI-R) that identifies an offender’s risk of reoffending and their criminogenic needs (Watkins 2011). LSI-R is described as a third generation risk assessment tool that balances static factors with dynamic factors and has capacity to identify need patterns and profiles of offender groups. It is ‘regarded as a good predictor of general reoffending, but also a modest predictor of violence. Its capacity to predict sexual reoffending is mixed’ (State of New South Wales v Graham James Kay [2018] NSWSC 1235, Wilson J at [39]). Such judicial commentary provides insights into the limitations of such risk assessment tools. Indeed, the LSI-R instrument has been criticised for being framed by specific geographic and temporal inputs (Koepke and Robinson 2018) and on the basis that offenders do not have homogenous criminogenic needs or profiles (Hsu et al. 2009). Clearly much reliance is being placed on actuarial predictive tools in the risk assessment of offenders at many levels of the criminal justice system (Carlson 2017) and multiple forms of scoring risk are evident in the case law. Nevertheless there are recognised shortcomings. The assessing psychologist in State of NSW v Dillon (Final) recognised the limitations of the STATIC-99R or STATIC-2002R instruments being that the recidivism estimates and rankings are based on groups of individuals, not necessarily directly reflective of the particular offender. He told the court: When comparing group data to individual cases it is important to note that factors and circumstances unique to an individual may not have been captured within the normative group and caution must be exercised when making such a comparison. (State of New South Wales v Dillon (Final) [2018] NSWSC 1626, at [102]). Moreover, the assessing psychologist explained that the tool may predict forms of sexual reoffending that would not actually even fall within the ambit of ‘serious’ sexual offences. Such actuarial instruments produce scores that ‘do not differentiate between the severity of offences that might be committed’, for example, they do not distinguish between grievous bodily harm and assault occasioning actual bodily harm (State of New South Wales v King (Final) [2019] NSWSC 151, at [142]). STATIC instruments are also not sensitive to any changing circumstances that may positively or negatively affect the offender’s actual risks of reoffending (State of New South Wales v Cook (Final) [2019] NSWSC 51). Nevertheless, findings from tools such as the STATIC-99R are often combined with other tools such as the STABLE-2007 to present a composite Risks-Needs-Responsivity (RNR) assessment that focuses on risk management in the structuring of interventions, treatment and support (Smid 2014; Eckhouse et al. 2019; Moore 2015). The statistical likelihood of reoffending may also scored through other instruments including the Violence Risk Appraisal Guide – Revised (VRAG-R) and the Violence Risk Scale (Grann and Långström 2007). On the face of it, it seems that these actuarial or algorithmic instruments offer evidence-based and objective forms of risk assessment. However, a number of critical concerns have been expressed. Jones and Milton (2016: 1) note the advantages of statistical methods especially in dealing with large caseloads but importantly acknowledge that algorithms provide ‘very little information about the actual individual being assessed’ thus recognising the conflict with the principle of individualised justice. Other studies have found that the margins of error of these actuarial or mathematical methods are large and the application of group data to an individual cannot be meaningfully undertaken with precision (Hart et al. 2007). Several scholars identify the opacity of the algorithm as being particularly problematic, that is, the actual algorithm, its inputs or processes may be protected trade secrets so that individuals impacted by the algorithmic assessment cannot critique or understand the determination (Hogan-Doran 2017; Carlson 2017). For example, is it possible to question the exact weighting applied to various risk factors to understand if the weighting is excessive or disproportionate to other factors? How can individuals respond to the case brought against them, challenge the accuracy of the algorithm and defend themselves against an adverse determination? At the end of the day, are there decisions or assessments that critically impact individual human lives and liberty that should not be delegated to algorithms? Automated systems and algorithmic assessments have become ‘a largely uncontested aspect’ and de rigeur in criminal procedure (Carlson 2017: 313 quoting Harcourt 2005) yet they give rise to questions as to exactly who - or what - is now the primary decision-maker (Hogan- Doran 2017). Algorithms and Embedded Bias The US case of State of Wisconsin v Loomis 881 N.W.2d 749 (Wis. 2016) illustrates the challenges in utilising algorithmic risk assessment tools, such as the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), in sentencing procedure (Gordon 2017: Harvard 2017). COMPAS, designed by Northpointe (now Equivant), a private for-profit company, is used to assess an offender’s criminogenic needs and predict the likelihood of their reoffending (Eckhouse et al. 2019; Carlson 2017). It is used widely in the US by various justice agencies ‘to inform decisions regarding the placement, supervision and case management of offenders’ (Northpointe 2012). Pre-sentencing, Mr Loomis was assessed by COMPAS as presenting a high risk of reoffending and ultimately sentenced to six years imprisonment and five years of extended supervision. He appealed against the severity of the sentence and asserted that the COMPAS tool breached his right to be sentenced according to accurate information, it breached his right to an individualised sentence, and it improperly used gendered assessments (Carlson 2017). On appeal, the Supreme Court held that the defendant’s right to due process was not violated by the use of a risk assessment scoring system. Loomis raises the issues of profiling an individual against a predictive algorithm and concerns regarding the biases embedded within. There is unease that algorithmic tools may disproportionately score some offenders, particularly from marginalised communities, as having a higher risk of reoffending, reinforcing dominant social hegemonies, prejudice and inequality (Martin 2017; Angwin et al. 2016). Broad (2018) examines how the over-policing of black populations in the US fashions the algorithms used in criminal justice. Eckhouse et al. (2019) examine the layers of embedded bias in statistical risk assessments – how the process of using data from an already racially biased criminal justice system perpetuates bias in the resultant risk scores. In relation to the COMPAS tool used in Loomis, that instrument was found by ProPublica to be ‘nearly twice as likely to inaccurately predict that a Black defendant was at high risk for rearrest as a White defendant’ (Eckhouse et al. 2019: 190 quoting Angwin et al. 2016), that is, it can falsely label and misclassify Black defendants as criminals (Carlson 2017). These conclusions have been disputed (Northpointe 2016; ProPublica 2016; Tashea 2017). Moreover in Loomis, the risk factor in question was gender, not race, and it was argued, unsuccessfully, that due process was violated by the use of a risk assessment tool in which gender was a factor. Ultimately, the use of COMPAS in Loomis was seen as acceptable given that it was not the sole determinative means to assess the risk of reoffending. However, the judgment did acknowledge the dangers in placing complete reliance on algorithmic instruments in decisions concerning liberty (Eckhouse et al. 2019). Algorithms as Proprietary Products Loomis also reveals the impenetrability of algorithms that are subject to proprietary interests or trade secrets (Gordon 2017). While the offender in Loomis could verify certain inputs to the risk assessment, the concluding score that recommended a significant prison sentence could not be challenged as the internal structure and formulae of the COMPAS algorithm was based on proprietary information. The Court accepted that Northpointe treated COMPAS as a trade secret and therefore it did not need to disclose how risk scores or factors were determined and weighed (Carlson 2017). Therefore Mr Loomis was not entitled to access the formulae or factors that were determinative in his high risk score and his significant punishment. As the Honourable Justice Martin argues, ‘this is entirely inconsistent with the common law requirement that a decision maker must expose his or her reasoning’ (Martin 2017: 22). Profiling an individual’s behaviour and characteristics against meta-data is also problematic as the process is largely invisible and its validity is difficult to challenge. Risk assessment tools are constantly evolving and must be monitored and ‘re-normed for accuracy due to changing populations and subpopulations’ (Loomis: 769). According to Hogan-Doran (2017: 13 quoting Goodfellow et al. 2017), the internal structure of algorithms, especially those based on Deep Learning, are virtually impossible to decipher given the cascade of layers of processing units. This leads to the ‘black box’ problem where the inputs and outputs may be clear but the process remains opaque. Eckhouse et al. (2019: 16) critique the Loomis decision as failing to address the trade secret of the COMPAS instrument, thereby preventing ‘judges, defendants, and researchers from vetting the algorithms and evaluating the fairness’. Carlson (2017: 329) argues that the use of risk assessment tools such as COMPAS in criminal justice should be subject to the ‘same transparency requirements as other government agencies’ instead of protecting the commercial interests of the private vendor. Interestingly, the Supreme Court in Loomis held that future risk assessments using COMPAS must disclose the proprietary nature of the instrument and how its scores are based on group data from a national sample, rather than the particular individual. Whether that warning will successfully encourage a degree of scepticism in judges regarding the use of risk assessment instruments is debatable, given the broad endorsement of these tools throughout the criminal justice system and the efficiency pressures placed on the judiciary (Harvard 2017). Clearly the algorithmic instruments used to predict risks of reoffending are largely sealed, secret and autonomous. In this way, it is argued that the proprietorial nature of algorithms created by private organisations challenges the fundamental principles of procedural justice, particularly, open justice and individualised justice. Technologised Procedural Justice As a range of technologies intrude and transform legal procedure and practice, there is an imperative to scrutinise the ethics of using algorithmic tools in the context of the fundamental principles of procedural justice. Is it possible to reconcile new technologies with traditional common law principles, professional ethics and human rights (Martin 2017; AHRC 2018; Law Society 2019)? When risk assessment tools are analysed through the lens of procedural justice principles, it quickly becomes evident how algorithmic instruments and automated systems that make decisions about people’s lives and liberty may compromise procedural justice. Procedural justice, a slippery term that encapsulates fairness and due process, derives from natural justice and its elements include open justice, equality before the law, the presumption of innocence and the right to hear and answer a case brought by the state (McKay 2018; Bronitt and McSherry 2017; Mulcahy 2013). According to open justice, criminal proceedings should be subject to public oversight as a means to counteract abuses of power and to promote transparency, accountability and ultimately, the rule of law (Resnik 2015). Open justice may be undermined when defendants, courts and society are denied the oversight of algorithmic tools that are used in determining a defendant’s legal status and liberty. Such tools need to be ‘testable and contestable’ (Hildebrandt 2018: 34). Equality of arms is a key principle in procedural justice meaning that the defendant should not be at a disadvantage compared with the prosecuting state, that is, there should be a level playing field (Roberts and Zuckerman 2010). Of course, in criminal procedure, that principle represents the ideal rather than the reality, nevertheless it is further challenged in situations where the prosecution uses inscrutable algorithmic tools and undisclosed input data against a defendant. The presumption of innocence is the golden thread that runs through the criminal justice system to ensure that the legal onus of proof remains on the prosecution (Woolmington v DPP [1935] AC 462). If there is no way of proving or disproving an algorithm’s formulae or methodology, the burden of proving a case against a defendant beyond reasonable doubt seems compromised. Additionally, both the presumption of innocence as well as the principle of individualised justice (Martin 2017) are potentially undermined when an individual defendant is assessed against aggregate group data. Finally, the hearing rule, audi alteram partem, requires that the defendant be enabled to hear and comprehend the case being brought against them (Butt and Hamer 2011) yet the use of secret proprietary information against a citizen is at odds with this right. These issues can also be assessed through the lens of human rights principles (AHRC 2018; Pasquale and Cashwell 2018; Aletras et al. 2006; ICCPR). For instance, there is the potential for algorithmic instruments to violate human rights, specifically, equality before the courts and the right to a fair and public hearing heard by a competent, independent and impartial tribunal. Other human rights measures that may be challenged by AI include the presumption of innocence, general procedural fairness requirements including the presentation of an understandable case against the defendant, and protection from discrimination. A report in England and Wales highlights these challenges to procedural justice as well as embedded bias, lack of scrutiny and the disregard of individual contextual factors (Law Society 2019). Rather than as an after- thought, procedural safeguards therefore need to be considered before the cautious implementation of algorithmic assessments that ‘score’ a person and determine their liberty or legal status. Procedural justice demands that persons affected by an algorithmic determination should be enabled to unpack and contest the decision (Keats Citron and Pasquale 2014). The suppression of algorithms’ operation and structure means that courts, judiciary, legal profession and defendants are denied the ability to comprehend and contest the decision-making process. Legitimacy and the Rule of Law The diminution of procedural justice values can be seen as ultimately undermining the legitimacy of criminal process. A 2019 Australian Discussion Paper identified a range of core AI principles including ‘Generates net-benefits’, ‘Do no harm’, ‘Regulatory and legal compliance’, ‘Privacy protection’, ‘Fairness’, ‘Transparency & Explainability’, ‘Contestability’ and ‘Accountability’ (Dawson et al. 2019: 6). What is missing is the principle of legitimacy that underpins society’s voluntary compliance and trust in the law. Legitimacy is premised on a ‘moral authority, which in turn depends on law’s ability to justify its requirements’ (Stern 2018: 4 quoting Sheppard 2018 and Raz 1979; McKay 2019). It is associated with the rule of law that, amongst other things, requires legal decision-makers to have authority to determine outcomes and for the decision- making process to be examinable and contestable (Oswald 2018). In addition, the rule of law provides for legal certainty, as well as checks and balances that enable citizens to enforce fundamental rights (Hildebrandt 2018). To fulfil legitimacy, the law must be ‘accessible and so far as possible intelligible, clear and predictable’ (Gordon 2017: 2 quoting Lord Bingham 2006). However, it is clear that when responsibility for a decision is delegated to an algorithmic instrument, that delegation can render decision- making opaque, inscrutable and incontestable (Hildebrandt 2018). On this basis, this article has sought to demonstrate some of the ways that the legitimacy of criminal procedure is challenged when decision-making authority is ‘ceded to the algorithm’ (Stern 2018: 3). Impacts of Risk Scores on Decision-makers Can the mere existence of a risk assessment score, even as complementary methodology, sway a human decision-maker? Eckhouse et al. (2019) suggest yes; a predictive risk score can influence a judge’s decision by focusing attention on potential recidivism over and above other relevant factors. For example, Carlson (2017) discusses a case where the COMPAS score was so high that the sentencing judge overturned the plea deal and sentenced the offender to two years, whereas the judge acknowledged that without the risk assessment, he would have only imposed a one year sentence. In addition, risk-adverse judges may rely on risk scores as a means to deflect blame onto the algorithm, in effect the risk assessment tool acts to ‘de-reponsibilize decision- makers’ (Harcourt 2010); Carlson 2017; Eckhouse et al. 2019). Other studies suggest that it would be unusual for a judge to defy the algorithmic conclusion (Harvard 2017; Christin et al. 2015). Indeed, given the proprietorial claims made by vendors of algorithmic tools, it would be impossible for a judge to meaningfully critique or challenge the risk assessment. Ultimately, judges can be ‘questioned and rebuked for discriminatory behaviour’ (Pasquale and Cashwell 2018: 66) ‘whereas an algorithm subtly premised on biased data … could remain virtually immune from criticism’ (Stern 2018: 6). While earlier in this article I discussed criticisms of judicial decision-makers and their perceived human ‘subconscious bias’ (Stobbs et al. 2017: 262), embedding algorithmic unconscious processes into decision-making does not address transparency concerns; it serves only to replace human intuition with mechanical inscrutability and incontestability (Oswald 2018; Hildebrandt 2018; Stern 2018). Conclusion With the potential for algorithmic instruments to ‘deeply change the nature of the evolution of the law’ (Law Society 2019: 4), there is a recognised need for responsible, accountable and ethical algorithmic design (Kroll 2015) and suggestions that, instead of placing reliance on the private commercial sector, governments ought to develop their own actuarial and algorithmic instruments (Carlson 2017). But in countries such as Australia, where the separation of powers between executive and judicial functions is valued, that could lead to an unacceptable blurring of that separation. Certainly, various scholars support the idea of a regulatory body to oversee and audit algorithms and thereby ensure transparency, accountability and procedural justice (Hogan-Doran 2017; Pasquale 2017; Balkin 2017). For example in England and Wales, a National Register of Algorithmic Systems has been recommended (Law Society 2019). Various scholars argue that where private, commercial organisations are involved in essential public functions, their products should be subject to public, democratic disclosure and freedom of information requirements (Carlson 2017; Keats Citron and Pasquale 2014). Criminal justice is a human institution – it is focused on human behaviours and human harms and has, traditionally, resolved human transgression in a communal fashion. While traditional, non-technologised procedure cannot be valourised, the legitimacy of the criminal justice system, and particularly the coercive power of the state to punish, imprison and supervise offenders, has been premised on open justice, a system that aspires to accountability, impartiality and transparency. The incursion of secret algorithms devised by the private for-profit sector into the public duties of judicial officers challenges the presumed independence of judicial functions. While algorithmic instruments may be useful and complementary predictive tools, they have no role as a sole or final arbiter. To invoke proprietorial protections and financial interests is to prohibit defendants, courts and the community from scrutinising the validity and reliability of predictive formulae used in deterministic criminal procedures. The situation thus synthesizes an element into decision-making that is even more opaque than any exercise of judicial discretion. At least an imperfect decision by a judge may be tested on appeal, whereas an imperfect algorithm may be forever concealed. References Aletras, N et al, (2006) ‘Predicting Judicial Decisions of the European Court of Human Rights: A Natural Language Processing Perspective’ 2 PeerJ Computer Science 92 Angwin, J, Larson, J Mattu, S and Kirchner, L (2016) Machine bias. ProPublica. sentencing Anthony, T., Bartels, L. and Hopkins, A. (2015). Lessons Lost in Sentencing: Welding Individualised Justice to Indigenous Justice, Melbourne University Law Review. Vol 39: 47 Ashworth, A and Zedner, L (eds) (2014), Preventive Justice, OUP Oxford Australian Human Rights Commission (AHRC) (2018) Human Rights and Technology Issues Paper, July 2018. Human-Rights-Tech-IP.pdf Australian Law Reform Commission (ALRC) (2005) Sentencing of Federal Offenders Discussion Paper (No 70, 2005) Balkin, J (2017) The Three Laws of Robotics in the Age of Big Data. Ohio State Law Journal Vol 78: 5, 1217. e.com.au/&httpsredir=1&article=6160&context=fss_papers Barabas, C., Dinakar, K., Ito, J., Virza, M., & Zittrain, J. (2017). Interventions over predictions: Reframing the ethical debate for actuarial risk assessment. arXiv preprint arXiv:1712.08238 Beck, A (1992) Risk Society, Towards a New Modernity Ulrich Beck. Broad, E (2018) Made by Humans: The AI Condition Melbourne University Press Bronitt, S and McSherry, B (2017) Principles of Criminal Law. Lawbook Company Brown, D, Farrier, D, McNamara, L, Steel, A, Grewcock, M, Quilter, J and Schwartz, M (2015) Criminal Laws, The Federation Press, 43-44 Brown, D. and Quilter, J. (2014) ‘Speaking too soon: The sabotage of bail reform in NSW’ International Journal for Crime, Justice and Social Democracy, 3(3), 4-28 Butt, P and Hamer, D (2011) LexisNexis Concise Australian Legal Dictionary LexisNexis Butterworths Carlson, A (2017) ‘The Need for Transparency in the Age of Predictive Sentencing Algorithms’, Iowa Law Review, Vol. 103: 303-329 Christin, A, Rosenblat, A and Boyd, D (2015) Courts and Predictive Algorithms Data & Society 1027/Courts_and_Predictive_Algorithms.pdf Dawson, D. and Schleiger, E., Horton, J., McLaughlin, J., Robinson, C., Quezada, G., Scowcroft, J., and Hajkowicz, S. (2019). Artificial Intelligence: Australia’s Ethics Framework. Data61 CSIRO, Australia policy/artificial-intelligence-ethics-framework/ Eckhouse, L., Lum, K., Conti-Cook, C. and Ciccolini, J. (2019) Layers of Bias: A unified approach for understanding problems with risk assessment. Criminal Justice and Behavior, vol. 46, No. 2, 185-209 European Commission High-Level Expert Group on Artificial Intelligence (2019). Ethics Guidelines for Trustworthy AI. market/en/news/ethics-guidelines-trustworthy-ai Goodfellow, I, Benigo, Y and Courville, A (2017) Deep Learning, MIT Press The Hon. M. Gordon, 2017. Courts and the Future of the Rule of Law justices/gordonj/gordonj21Jul2017.pdf Grann, M and Långström, N (2007) Actuarial Assessment of Violence Risk: To Weigh or Not to Weigh? Criminal Justice and Behaviour, Vol. 34, No. 1, January, 22-36 Gray, N., Laing, J. & Noaks, L. (2002). Criminal justice, mental health and the politics of risk. London: Cavendish Publishing Ltd Harcourt, B (2005) Against Prediction: Sentencing, Policing, and Punishing in an Actuarial Age, University of Chicago Public Law & Legal Theory Working Paper No. Harcourt, B (2010) Risk as a Proxy for Race 2 (John M Olin & Econ Working Paper No 535 (2d Series) & Pub. Law & Legal Theory Working Paper No 323, 2010) _law_and_legal_theory Hart, SD, Michie, C, and Cooke, DJ (2007). Precision of Actuarial Risk Assessment Instruments: Evaluating the “margins of error” of group v. individual predictions of violence. British Journal of Psychiatry. 190(49), 60-65 Harvard Law Review Case Note (2017). State v. Loomis: Wisconsin Supreme Court Requires Warning Before Use of Algorithmic Risk Assessments in Sentencing. Harvard Law Review. Hildebrandt, M. (2018). Law as computation in the era of artificial legal intelligence: Speaking law to the power of statistics. University of Toronto Law Journal, 68(supplement 1), 12-35, 34 Hobbs, H and Trotter, A (2018) UNSW Law Journal Vol 41(2): 319-354 Hogan-Doran, D (2017) ‘Computer says ‘no’: Automation, algorithms and artificial intelligence in Government decision-making’, 13 The Judicial Review Hsu, CI, Caputi, P, and Byrne, MK (2009). The Level of Service Inventory—Revised (LSI-R) A Useful Risk Assessment Measure for Australian Offenders?. Criminal Justice and Behavior, 36(7), 728-740 Husak, D (2008) Overcriminalization: The Limits of the Criminal Law, Oxford University Press Jones, L and Milton, E (2016). Risk of Sexual Violence Protocol (RSVP): A real world study of the reliability, validity and utility of a structured professional judgement instrument in the assessment and management of sexual offenders in South East Scotland tocol_RSVP_A_real_world_study_of_the_reliability_validity_and_utility_of_a_stru ctured_professional_judgement_instrument_in_the_assessment_and_management_of _sexual_offenders_ Keats Citron, D and Pasquale, F (2014) The scored society: due process for automated predictions. Washington Law Review 89:1. law/bitstream/handle/1773.1/1318/89WLR0001.pdf Koepke, J L and Robinson, DG (2018). Danger ahead: Risk assessment and the future of bail reform. Wash. L. Rev., 93, 1725 Krasnostein, S and Freiberg, A, “Pursuing Consistency in an Individualistic Sentencing Framework: If You Know Where You’re Going, How Do You Know When You’ve Got There?” (2013) 76 Law and Contemporary Problems 265 Kroll, J (2015) Accountable Algorithms. PhD Thesis. Princeton University. Law Society Commission on the Use of Algorithms in the Justice System and The Law Society of England and Wales. (2019). Algorithms in the Criminal Justice System, June 2019. The Law Society of England and Wales. the-criminal-justice-system-report/ Lee, M (2007). Inventing Fear of Crime: Criminology and the Politics of Anxiety. United Kingdom: Willan Publishing Lord Bingham, "The Rule of Law", speech delivered as the Centre for Public Law's Sixth Sir David Williams Lecture, 16 November 2006 Martin, GC (2017) How far has technology invaded the criminal justice system? McSherry B. (2004). Risk assessment by mental health professionals and the prevention of future violent behaviour. Trends & issues in crime and criminal justice No. 281. Canberra: Australian Institute of Criminology. McKay, C (2018) The Pixelated Prisoner: Prison video links, court ‘appearance’ and the justice matrix. Routledge McKay, C (2019). Submission to the Department of Industry, Innovation and Science Discussion Paper on Artificial Intelligence: Australia’s Ethics Framework Moore R (ed), (2015). A Compendium of Research and Analysis on the Offender Assessment System (Ministry of Justice Analytical Series 2015) Mulcahy, L (2013) Putting the defendant in their place: Why do we still use the dock in criminal proceedings? British Journal of Criminology 53(6), 1139-56 Northpointe (2012) Practitioners Guide to COMPAS Northpointe (2016) COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity, July 2016. ProPublica-Commentary-Final-070616.html#document/p32/a310125 O’Malley, P (2013) The Politics of Mass Preventive Justice in (Ashworth, A, Zedner, L and Tomlin, P (eds) Prevention and the Limits of the Criminal Law, Oxford University Press, 273-4, 276 Oswald, M (2018). Algorithm-assisted decision-making in the public sector: framing the issues using administrative law rules governing discretionary power. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 376(2128), 20170359 Pasquale, F (2017) Toward a Fourth law of Robotics: Preserving Attribution, Responsibility, and Explainability in an Algorithmic Society. Ohio State Law Journal, Vol 78, U of Maryland Legal Studies Research Paper No. 2017-21. Available at SSRN: Pasquale, F and Cashwell, G (2018) ‘Prediction, Persuasion, and the Jurisprudence of Behaviourism’ 68:Suppl University of Toronto Law Journal 63 ProPublica (2016) Technical Response to Northpointe Raz, J., The Authority of Law: Essays on Law and Morality (Oxford: Clarendon Press, 1979) Resnik, J (2015) The contingency of courts: changing the experiences and logics of the public’s role in court-based ADR Nevada Law Journal 15, 951 Roberts, P and Zuckerman, A (2010) Criminal Evidence. Oxford University Press Sheppard, B (2018) ‘Warming up to Inscrutability: How Technology Could Challenge Our Concept of Law’ 68: Suppl UTLJ 36 Smid, WJ (2014) Sex offender risk assessments in the Netherlands: Towards a risk need responsivity oriented approach. PhD Thesis Sourdin, T (2018) Judge v Robot? Artificial Intelligence and Judicial Decision-Making UNSW Law Journal Vol 41(4): 1114-1133 Stern, S (2018). Introduction: Artificial intelligence, technology, and the law. University of Toronto Law Journal, 68(supplement 1), 1-11 Stobbs, N, Hunter, D and Bagaric, M (2017). ‘Can Sentencing Be Enhanced by the Use of Artificial Intelligence?’ 41 Criminal Law Journal 261 Tashea, J (2017) Risk-assessment algorithms challenged in bail, sentencing and parole decisions. ABA Journal. March 1, 2017. Watkins, I, (2011) The Utility of Level of Service Inventory – Revised (LSI-R) Assessments within NSW Correctional Environments (29 January 2011) NSW Department of Corrective Services service-inventory-.pdf Wexler, R. (2017). Code of Silence. How private companies hide flaws in the software that governments use to decide who goes to prison and who gets out. Washington Monthly. silence/ Zdenkowski, G (2000) Limiting Sentencing Discretion: Has There Been a Paradigm Shift, 12 Current Issues Crim. Just. 58 Zedner, L (2007). Pre-crime and post-criminology? Theoretical Criminology, 11(2), 261-281 Case Law Bugmy v The Queen [2013] HCA 37; (2013) 249 CLR 571 Dennis v United States 339 US 162, 184 (Frankfurter J) (1950) Elias v The Queen (2013) 248 CLR 483 Haidy v DPP [2004] VSC 247 Hili v The Queen (2010) 242 CLR 520 Hudson v The Queen (2010) 30 VR 610; 205 A Crim R 199; [2010] VSCA 332 Lynn v State of New South Wales [2016] NSWCA 57 Markarian v The Queen [2005] HCA 25 Muldrock v The Queen [2011] HCA 39 Munda v Western Australia (2013) 249 CLR 600 R v Elzamtar [2017] NSWSC 275 Williams v DPP (Qld) [1999] QCA 356 R v Agang; R v Bajwa; R v Ghanem [2017] NSWSC 138 R v Fernando (1992) 76 A Crim R 58 State of New South Wales v Barrie (Final) [2018] NSWSC 1005 State of NSW v Ceissman [2018] NSWSC 508 State of New South Wales v Cook (Final) [2019] NSWSC 51 State of New South Wales v Dillon (Final) [2018] NSWSC 1626 State of New South Wales v Graham James Kay [2018] NSWSC 1235 State of New South Wales v King (Final) [2019] NSWSC 151 State of New South Wales v Pacey (Final) [2015] NSWSC 1983 State of Wisconsin v Loomis 881 N.W.2d 749 (Wis. 2016) Veen (no. 1) (1979) 143 CLR 458 Veen (no. 2) (1988) 164 CLR 465 Wong v The Queen [2001] HCA 54 Woolmington v DPP [1935] AC 462 Legislation and Conventions Bail Act 2013 (NSW) Crimes (Administration of Sentences) Act 1999 (NSW) Crimes (Administration of Sentences) Regulation 2014 (NSW) Crimes (High Risk Offenders) Act 2006 (NSW) International Covenant on Civil and Political Rights (ICCPR) opened for signature 16 December 1966, 999 UNTS 171 (entered into force 23 March 1976) Terrorism (High Risk Offenders) Act 2017 (NSW)