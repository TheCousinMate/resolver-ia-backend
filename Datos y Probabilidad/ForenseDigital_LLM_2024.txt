Abstract Kyung-Jong Kima,1, Chan-Hwi Leeb, So-Eun Baec, Ju-Hyun Choid, Wook Kange,2 Keywords: Artificial intelligence, Digital Forensics, Instant Messenger, Large Language Model(LLM) Digital Forensics in Law Enforcement: A Case Study of LLM-driven Evidence Analysis aKorea Advanced Institute of Science and Technology, Daehak-ro 291, Yuseong-gu, Daejeon, 34141, Republic of Korea bCybersecurity Research Center, Korean National Police University, Hwangsan-gil 100-50, Asan-si, 31539, Republic of Korea cPublic Safety Data Science Research Center, Korean National Police University, Hwangsan-gil 100-50, Asan-si, 31539, Republic of Korea dPublic Safety Data Science Research Center, Korean National Police University, Hwangsan-gil 100-50, Asan-si, 31539, Republic of Korea eKorean National Police University, Hwangsan-gil 100-50, Asan-si, 31539, Republic of Korea The advent of digital technology and the ubiquity of mobile devices in today’s society has led to a significant increase in the importance of mobile forensics in criminal investigations. Responding to the escalating volume and complexity of data due to enhanced smartphone capabilities and pervasive messaging apps, law enforcement agencies face challenges in data analysis. This study explores improving investigative efficiency through LLM-driven analysis of text from mobile messenger communications. We have conducted experiments on anonymized data collected from real crime scenes by employing three state-of-the-art LLM models, namely GPT-4o, Gemini 1.5 and Claude 3.5. The study focuses on optimizing model performance by employing prompt engineering, interpreting expressions embedded with hidden meanings such as slang, and contextually inferring ambiguous word usage. Finally, model performance is quantitatively evaluated using metrics such as precision, recall, F1 score, and hallucination rate. Preprint not peer reviewed Email addresses: leeyeongul@kaist.ac.kr (Kyung-Jong Kim), kangw@police.ac.kr (Wook Kang) 1 Corresponding author 2 Corresponding author This preprint research paper has not been peer reviewed. is and future research The current 1. Introduction to be analyzed 2. Related Works (such as money investigative practice of The field of digital forensics has continuously evolved with the artificial intelligence(AI) technologies, leading to numerous research initiatives focused on developing sophisticated tools and methods to enhance forensic analysis and criminal investigations. Mukkamala and Sung (2003) pioneered AI integration in network forensics through offline intrusion analysis, while Case et al. (2008) established automated correlation analysis between digital evidence. Hoelz et al. (2008) advanced this field by developing multi-agent systems and casebased reasoning approaches. enhance both the efficiency and accuracy of digital crime investigations, providing investigators with more powerful and intuitive tools for evidence analysis. • Our research presents detailed experimental designs and results, along with a critical discussion of limitations directions, contributing to the methodological advancement of mobile forensics investigation techniques. With the advancement of digital technology and mobile devices that permeate all aspects of daily life, the importance of mobile forensics in criminal investigations has increased significantly (Alshameri et al., 2024). As smartphone storage capacity increases and various messenger apps become widespread, law enforcement agencies are facing an era in which the volume and complexity of data increasing exponentially (Sj¨ostrand, 2020). In this context, this study proposes a method to improve investigative efficiency by applying LLM technology to the analysis of text from mobile messenger messages. identifying crimerelated keywords transfers, transaction objects, locations) through keyword searches is a prevalent method. This study specifically examines through experiments, the extent to which LLMs can contribute to the extraction of key messages related to criminal contexts through message context analysis. The experiments were conducted using anonymized data collected from actual crime scenes, and three state-of-the- art LLM models – GPT-4o, Gemini 1.5, and Claude 3.5 – were used to identify the most effective model in the field of mobile forensics. Preprint not peer reviewed In this process, efforts were made to maximize the models’ through prompt interpreting expressions with hidden engineering, intentions such as slang, and inferring the usage of words through context when ambiguous expressions appear. Finally, the models’ performance is quantitatively evaluated through metrics such as Precision, Recall, F1 score, and Hallucination Rate. Our research makes several significant contributions to the field of digital forensics, particularly in the area of mobile device investigations using real-world police data. Through comprehensive analysis of actual criminal investigations, we propose a novel mobile forensics framework that enhances the efficiency and accuracy of digital evidence analysis through the integration of AI and data visualization technologies. • We used over 140,000 real-world forensic messages from mobile phones in drug-related investigations by the Korean National Police Agency, with Ground Truth established by experienced investigators. This practical approach validates the real-world applicability of our proposed methodology in actual investigation environments. Fayyaz et al. (2024) developed a hybrid AI framework for analyzing vehicle infotainment system data, while Yang et al. (2020) demonstrated CNN’s effectiveness in image source forensics. Vasilaras et al. (2024) evaluated AI-based mobile forensics tools, emphasizing transparency and ethical considerations. Afchar et al. (2018) and Orozco et al. through deepfake (2020) advanced video detection and post-processing analysis respectively. Scanlon et al. (2023) evaluated ChatGPT’s potential in digital forensics across six domains, highlighting its utility as a supplementary tool while noting limitations requiring expert oversight. Oh et al. (2024) introduced volGPT, successfully for explainable memory forensics triage with high accuracy in ransomware detection. Law enforcement agencies always operate under time constraints. For example, if a criminal is tracked down, apprehended, and arrested in South Korea, and you want to keep him or her in custody, you have 36 hours to apply for an arrest warrant that more specifically defines the charges that against These studies collectively demonstrate AI’s crucial role across digital forensics domains, emphasizing both its potential and the importance of maintaining legal validity and reliability. In recent developments, Qamhan et al. (2021) applied CRNN models to audio forensics, while Rughani (2017) proposed a comprehensive AI-driven forensics framework integrating collection, analysis, and reporting stages. • We developed a novel framework that integrates AI and data visualization technologies to significantly implementing prompt-based LLMs 3. Research Problem and Proposals identification performance Preprint submitted to FSI: DI submission includes evidence 1.1. Contribution suspect, and January 2, 2025 forensics seized the This preprint research paper has not been peer reviewed. 4.1. Framework Overview them to digital forensics through various experiments with the actual data provided by law enforcement agencies. 4. Framework We propose a framework that can help investigators to efficiently detect criminal clues to prove criminal offenses during digital forensics, thereby reducing time needed. The overall architecture is shown in Figure 1. This starts by acquiring mobile device data using some forensic softwares from mobile devices such as smartphones, SD memory cards, and IoT devices. This kind of software enables us to download the result file as preferred formats such as excel worksheet, csv, text, etc. Those output files from the software include evidence information, message history, and some metadata which is useful in the investigation process. demonstrates the suspect’s risk of fleeing the country, such as fear of destruction of evidence or fear that the suspect will flee again. Furthermore, even after this process, there is a time limitation as investigators must gather evidence and obtain suspect statements within 10 days before transferring the case to the prosecution. Meanwhile, the volume of data that law enforcement agencies must analyze to prove criminal charges is continuously expanding. Analyzing electronic information stored on suspects’ mobile phones has become mandatory rather than optional. As phone storage capacities increase, the total volume of data that investigators must examine continues to grow (Fayyaz et al., 2024). Particularly, with the development of various mobile messengers(Telegram, WeChat, Line, etc.), most people now communicate via mobile messengers rather than phone calls. Mobile messengers have become the most crucial communication channel in modern digital society, offering not only the ability to communicate through photos and videos but also various convenience features (money transfers, purchases, voting, etc.). This study aims to discuss methods for efficiently exploring such mobile messenger data within the aforementioned time constraints and extracting and analyzing information relevant to criminal charges. Preprint not peer reviewed Law enforcement agencies face major challenges when dealing with such extracted data. It requires us to put an enormous effort and substantial time to determine whether the extracted messages are genuinely related to certain crimes or a message refers to merely metaphorical use of ”drug” word (e.g., describing a song as ”addictive as drugs”). This requires investigators to analyze extensive amounts of adjacent messages to understand the context. One of the most widely used methods by law enforcement agencies to search for crime-related electronic information within mobile messengers is keyword searching. For example, when conducting forensics on a drug suspect’s phone, investigators extract message data through various mobile forensic tools and filter chats containing specific keywords like ”drugs” for investigation. • Solutions are needed to address the problem of investigators having to manually verify the context in which the message related to crime is located. To address these key issues, this study proposes the framework below: Utilizing SoTA LLM models to verify message context - examining the possibility of applying • Digital forensics procedures must be handled in a swift analyzing messenger data, can be summarized as follows: The key issues in mobile forensics, particularly in and accurate manner given such strict time limits. Fig. 1. Overview of LLM-driven Evidence Analysis Framework This preprint research paper has not been peer reviewed. 1 Tool Device Version iOS 15.6.1 iPhone 11 XS Android 10.2.2 Samsung Galaxy Note 10+ MD-RED 3.10.16.2360 MD-NEXT 2.0.14.2259 MD-RED 3.11.10.2964 4.2. Acquisition & Extraction Software Table 1: Software and device specifications used in the study applicable for feeding LLMs by pre-processing them. In the next step, we should make those output files A summary of the specific details regarding the software and smartphone models used in this study can be found in Table 1. Index I will provide you the target message and 40 additional messages from before and after the target message in a keyvalue format. Key is a nickname and value is the content of the conversation. On the basis of criteria above, make an overall judgment on whether or not the message and sender are involved in specific drug crimes and return the result. Below is the chat history for your reference.” Finally we utilize LLMs, such as ChatGPT, Claude, and fed and Gemini, reading all the context messages determining its relevance to the case. At this point, we apply prompt engineering techniques to finely tune the prompts, helping LLM to extract important and accurate information to deliver optimal results. The analysis of smartphone communication records is conducted using a two-step procedure. The first step is the physical extraction of data from the device, followed by the carving of files from the extracted binary data. While this process can be accomplished using open-source software, it typically requires more effort than using commercial tools. In this context, this study employs GMDSOFT’s MD-NEXT for data extraction and MD-RED for analysis, which are powerful tools for law enforcement agencies. Imagine that you’re an investigator looking into messages from the seized phones, and you have to identify the target message’s relation to the drug crime with clear and reasonable criteria. Please give me an output of 1 or 0, 1 refers to Positive and 0 refers to negative. You should mainly focus on whether the people in the conversation are directly involved in drug crimes(trafficking, drug use, or drug distribution, etc.). So just mentioning the offense of celebrities, politicians is negligible. Preprint not peer reviewed This prompt leads the model to act as an expertised investigator and, given the background information, to analyze the context in the message data and accurately extract important information related to the alleged crime. in the case of the GEMINI API, when the ’input text’ parts are directly connected, there exists a problem that the model forgot the instructions and focused on the ’input text’, which prevents the model from fulfilling its intended role and leads to the model focusing on the details of the input text and failing to identify important information related to the alleged crime. To address this issue, we implement the Sandwich Defense approach (Schulhoff, 2024) to ensure that the model does not forget instructions. This approach changes the structure of the prompt from ”instruction,” ”input text,” and ”reaffirmation of instruction” to help the model repeatedly recognize and retain the instruction. Understanding the context of messages from seized mobile devices is essential for investigation procedures. Since existing methods have limitations in effectively interpreting tabular data, we propose using SoTA LLMs, GPT-4o, Gemini 1.5, and Claude 3.5, to sift through pre- processed data. The prompts in our study are organized as follows: We specify the role of the model as a ”mobile forensic expert,” provide background information about the situation under investigation, and instruct the model to identify and extract text relevant to the alleged crime. For example, the prompt might be written in the following form: ”Determine whether the above text is related to drug crimes. For your reference, Firstly, we make keyword searches with certain terms, in this study we used ’drug’, and we have to get some additional messages from the chat rooms or chat opponents to ensure the message context. Thereby we get sufficient message sets which are appropriate to present to professionals or LLMs. After the anonymization, the whole dataset is too large to build ground truth or to directly feed it to LLMs. In addition, most messages are sent in pieces. So we performed pre- processing to tackle these issues and to make the dataset proper to be sent to LLMs. Prompts provide essential information for a model to accurately understand a given problem and perform appropriate analysis, and their structure and specificity are key determinants of the quality of a model’s response (Brown, 2020). Given that these models generate answers based on our input data, issues related to privacy and copyright infringement can be raised (Yang, 2023). In this perspective, we Comparing the specifications of the number of input and output tokens of the models used in this study, each model is characterized as shown in Table 2. 4.5. Contextual grasp with LLM 4.4. Prompt engineering 4.3. Pre-processing ’instructions’ and Max. Output Tokens Max. Input Tokens However, 128,000 GPT-4o 16,384 Model This preprint research paper has not been peer reviewed. 8,192 8,192 Model Name 2,088,959 200,000 Table 4: Main columns of the dataset Table 2: Model specifications comparison Gemini 1.5 Pro Claude 3.5 Sonnet cannot exclude the possibility that the sensitive data may be leaked to the outside. The three LLMs mentioned above have their own data policies about data leakage, reuse of input data as training purpose, data ownership, processing methods, and privacy measures. The details in Table 3. Column Name App Status Type Content Chat Room Group Sender Number Sender Receiver Receiver Number Creation Time Message ID Account threaded applications and large datasets. The system is equipped with 64 GiB of RAM, which ensures smooth performance without memory-related bottlenecks during complex calculations and data processing tasks. The workstation also features an NVIDIA GeForce RTX 2060 GPU, which has the ability to accelerate tasks such as training machine learning models and analyzing forensic data. Description Message application name Message status Message type Actual text content Unique chat room number Whether group chat Sender’s phone number Sender’s name Receiver’s name Receiver’s phone number Message creation time Unique message identifier Account information Example ’Message’, ’Telegram’, ’Kakao Talk’, ’Instagram’ ’Active’, ’Deleted’, ’Status’ ’Received’, ’Sent’, ’System Message’, ’Chat Room Setting’ ’Hello, sales will be made in order of arrival’ 242, 10092911026 ’True’ ’01012345678’ ’Ming Ming’, ’Comeboy’ ’Uni’, ’Leica’ ’114’, ’01011111111’ 2023-05-08 10:11:16 ’9D36093A-C864-E902-62E2-D695E5746403’ ’+821004704241’ Preprint not peer reviewed We use NER (Named Entity Recognition) to anonymize names and generate random numbers to replace contacts. In the case of emails, we replace all the usernames with the same amount of ’#’. It is also important to consider the possibility of generating the same number as existing contact when anonymzing contacts. Therefore, we generated random numbers within the range of middle digit numbers which are not being used in Korea to minimize the possibility of duplication with existing contacts. We acquired smartphones seized by law enforcement agencies from real world drug cases. Then we extract message data and export it into excel worksheet format as mentioned in Section 4.2. The whole message dataset used in this study consists of 31 columns and 142,214 rows. The main columns are shown in Table 4. We applied a series of data processing methods mentioned in our framework to real world drug cases. We extract the first 20 rows and the last 20 rows of the corresponding chat, and combine them into a single data set. In this way, we extract 200 data points from the first drug case and 200 data from the other drug case. As a result, we We anonymized personal information such as names, addresses and phone numbers to ensure privacy and to proceed for further research. (Enterprise) Input data is not used for training purposes. (Free, Plus) Can opt out not to use input data as a training purpose through data controls. The computing environment used in this study is a highperformance workstation specifically configured to support forensic data analysis tasks. The workstation is equipped with a 24-core Intel(R) Xeon(R) Gold 5220R CPU running at 2.20 GHz, providing powerful computational power for parallel processing. This configuration enables efficient processing of multi- Although we anonymized the original data in a way shown in Section 4.3, we also find the policy of each LLMs on data privacy stating that they will not use input data for training models when using commercial plans, especially for APIs. Does not use data (prompts or responses) to its models without your permission. (Anthropic API) Input data is not used for training purposes. 5.2. Dataset 5.2.1. Preview of the dataset Table 3: Data policies of LLM providers 5.2.2. Anonymization Claude 3.5 (Anthropic) 5.2.3. Pre-processing Gemini 1.5 (Google) 5.1. Environment 5. Experiments GPT-4o (OpenAI) intensive Policy train This preprint research paper has not been peer reviewed. (4) (3) (1) Where: • Expected Agreement ( The formula for Cohen’s Kappa (κ) is: agreements / Number of observations 5.2.4. Ground Truth & Cross Validation • Observed Agreement (Po) = Total number of is used to evaluate the ability to not miss important crime- related information. Recall is calculated as: get 400 excel workbook files where the keyword matching messages are presented with the contexts. We then measured Cohen’s Kappa (Cohen, 1960) to determine the quality of this annotation, and obtained a value of 0.74. Cohen’s Kappa is a statistical measure used to assess the degree of agreement between two raters (or annotators) in classifying items into mutually exclusive categories. To evaluate the performance of our framework, we need to label the dataset mentioned in Section 5.2 whether they are truly related to drug crimes or they are just including slang or some metaphorical use of certain words. We asked two experienced investigators for labeling each single dataset whether it is actually related to drug crime or simply referring to figure of speech. F1-score is a metric that evaluates the balance between Precision and Recall, and measures the overall performance of the model through a harmonized average of the two metrics. In particular, it is used to comprehensively evaluate how well the model understands the drug-related context and effectively categorizes the relevant sentences. The F1-score is calculated as: Hallucination refers to the phenomenon where a large language model (LLM) produces information that does not actually exist or incorrect information (Maynez et al., 2020). These errors can occur during the process of inference or deduction from the data on which the model was trained, and can be particularly problematic in fact-based tasks. In general, hallucinations can degrade the reliability and accuracy of a model, making them an important metric when evaluating model performance. We define a hallucination as a case where the LLM incorrectly judges the ground truth to be 1 when it is 0. Based on this definition, we calculate the Hallucination Rate as the rate at which the model produces incorrect outputs that do not match the actual data, and use this metric to assess whether the investigation. The model’s output Hallucination Rate is calculated as: Preprint not peer reviewed In our study, we use Precision, Recall, F1-score, and Hallucination Rate to evaluate how well large language models can discriminate between actual drug crimes and figurative expressions in message data. For example, a sentence such as ”This music is as addictive as drugs” is not relevant to the crime, but a sentence such as ”Let’s trade drugs for this price” is directly relevant to the crime. According to Powers (2020), Precision refers to the percentage of items positively predicted by the model that are actually correct, and is used to evaluate the ability of the model to accurately identify whether a particular sentence is text actually used in a drug crime or not. The formula for Precision is: Given that no model is superior in every metric, we introduce the majority vote system. In this system, the final result is a value of 1 (crime) if at least two of the above three it to be 1, and 0 otherwise. This models counterbalances the tendencies of each model to produce a more stable decision, resulting in high precision (0.944), moderate recall (0.835), and very low hallucination rate (0.077). Recall is the percentage of sentences that were actually used in a drug crime that the model correctly identified, and recall (0.782). • Claude-3.5 has the lowest accuracy (0.794) hallucination rate (0.186). • Gemini 1.5 is excellent when it truth annotated by the investigators as Table 5. • GPT-4o • Pi+ is the percentage of items assigned to category i by • P+i is the percentage of items assigned to category i by performs well on all metrics, especially recall (0.914) and F1 score (0.899). However, it does have a moderate comes to accuracy and hallucination-free, but has a low We compared the results from the LLMs to the ground According to Landis JRKoch (1977), if this number is We present graphic performance comparison in Figure 2. greater than 0.61, the data is of good quality. and the highest hallucination rate (0.346). • k is the number of categories 5.3. Comparison Metric 5.4. Experiment results annotator 1 annotator 2 is reliable in an judge (5) (2) This preprint research paper has not been peer reviewed. Model FP 0 12 FN 53 40 TP 190 203 TN 156 144 6.2. Limitations and Future Works F1 score 0.899 0.878 0.824 0.886 Precision 0.884 0.794 0.944 GPT-4o Gemini 1.5 Claude-3.5 Majority Vote Processing time 25m 33s 8m 31s 65m 59s - Hallucination rate 0.186 0.346 0.077 Recall 0.914 0.782 0.856 0.835 Table 5: Overall Performances of each models Although we discovered the usefulness of employing several SoTA LLMs in the crime investigation process, we still have some challenges in extracting information related to the case from mobile devices. Firstly, we keyword-searched for sentences containing the certain word ”drugs”, and created a set of messages by grouping preceding and following text messages. We then asked LLMs for judging their relevance to drug crimes based on the context of the message set. However this method excludes the possibilities of crime-related messages written in metaphorical terms or indirect terms. Therefore, Future research should move beyond simply keywordsearching the certain words, embed the entire data to more sophisticatedly analyze the semantic similarity between messages and the target words or sentences. We expect this method to overcome the limitations and extract more accurate information that takes into account more complex contexts. Preprint not peer reviewed Secondly, we experienced that our suggested framework is subject to each of the policies of LLMs. Sometimes they offer only limited responses to topics such as crime, violence, and drug dealing for user protection and safety. In addition, these LLMs prevent themselves from learning continuously updated laws or confidential investigative data (Kim et al., 2024), and they pose security concerns due to the risk of exposing sensitive information from criminal cases while using the API (Liu et al., 2023). To overcome these constraints and provide more accurate and secure customized responses for Law Enforcement Agencies, we should consider implementing a local model which is relatively small sized or customized for special fields of knowledge. Last but not least, prompt engineering has several limitations when applied to sensitive and context- dependent domains such as criminal investigation. In particular, the structure and wording of the prompts directly affect the performance of the model, and even simple structural changes can lead to significant changes in the generalization performance of the model (Schick and Schu¨tze, 2020; Zhao et al., 2021). When analyzing how effectively each model extracts information relevant to the alleged crime in real-world investigative situations, GPT-4o performed well, with the highest overall recall and F1 score. In contrast, Gemini had a lower recall but fewer false positives, and Claude had relatively low precision. The results of this performance analysis of each model are important to understand the strengths and limitations of each model, and we figure out that the majority voting method compensates for the conflicts between the models to produce stable and reliable results. This research presents a comprehensive framework for digital forensics, aiming to enhance the efficiency of investigators in sifting through crucial evidence for proving alleged crimes, thereby reducing investigation timeframes. The framework encompasses data extraction from diverse mobile devices using forensic software, preprocessing the obtained data, and applying prompt engineering to comprehend the context. This approach proves invaluable when rapid data analysis is required, facilitating the intuitive clues within constrained time limits. We can utilize a Retrieval Augmented Generation (RAG) model for an alternative (Gao et al., 2023). RAG models work by searching for relevant documents in realtime to add information that can be referenced when generating answers, rather than relying solely on pre-trained knowledge. When using the Gemini api, we found that the ’input text’ directly followed by the ’instructions’ caused the model to 6. Conclusion & Future work Fig. 2. Model Performance Comparison identification of 6.1. Summary significant This preprint research paper has not been peer reviewed. A. Case, A. Cristina, L. Marziale, G. G. Richard, V. Roussev, Face:Automated digital evidence discovery and correlation, digital investigation 5 (2008) S65–S75. S. Mukkamala, A. H. Sung, Identifying significant features for network forensic analysis using artificial intelligent techniques, International Journal of digital evidence 1 (2003) 1–17. B. W. Hoelz, C. G. Ralha, R. Geeverghese, H. C. Junior, A cooperative multi- agent approach to computer forensics, in: 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology, volume 2, IEEE, 2008, pp. 477– 483. F. Alshameri, K. Khanta, S. Boyce, A comparison study to analyse the data acquisitions of ios and android smartphones using multiple forensic tools, International Journal of Electronic Security and Digital Forensics 16 (2024) 267–283. M. Sj¨ostrand, Combatting the data volume issue in digital forensics: A M. Scanlon, F. Breitinger, C. Hargreaves, J.-N. Hilgert, J. Sheppard, Chatgpt for digital forensic investigation: The good, the bad, and the unknown, Forensic Science International: Digital Investigation 46 (2023) 301609. D. B. Oh, D. Kim, H. K. Kim, volgpt: Evaluation on triaging ransomware process in memory forensics with large language model, Forensic Science International: Digital Investigation 49 (2024) 301756. Y. Fayyaz, A. Almehmadi, K. El-Khatib, A hybrid artificial intelligence framework for enhancing digital forensic investigations of infotainment systems, Forensic Science International: Digital Investigation 49 (2024) 301751. P. Yang, D. Baracchi, R. Ni, Y. Zhao, F. Argenti, A. Piva, A survey of deep learning-based source image forensics, Journal of Imaging 6 (2020) 9. A. Vasilaras, N. Papadoudis, P. Rizomiliotis, Artificial intelligence in mobile forensics: A survey of current status, a use case analysis and ai alignment objectives, Forensic Science International: Digital Investigation 49 (2024) 301737. References structured literature review (2020). CRediT author statement forget about the instructions and focus on the input text, which resulted in the model failing to identify important crime-related information. We adopted the Sandwich Defense approach to settle this down. However, the problem remains. Kyungjong Kim: Project administration, Writing Original Draft, Conceptualization, Investigation, Validation, Data curation. Chanhwi Lee: Writing - Original Draft, software, Investigation, Validation, Data curation. Soeun Bae: Writing Investigation, - Review & Editing, Visualization, Validation, Data curation. Juhyun Choi: Writing - Review & Editing, Investigation, Validation, Formal analysis, Data curation. Wook Kang: Writing - Review & Editing, Validation, Supervision. For future work, we suggest an adaptive prompting framework to compensate for these issues, in which the prompts can be dynamically adjusted according to the complexity and context of the input data. For example, there is a need to design hierarchical prompts that help the model understand the context step by step instead of processing all the details at once (Schick and Schu¨tze, 2020). In addition, we hope to combine prompt engineering with few shot learning to design prompts with representative examples, which will allow the model to generalize in complex and diverse situations (Gao et al., 2020). Preprint not peer reviewed This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MIST) (No.RS- 2023-00225661, Development of causal reasoning and expression technology to enhance the proof of digital evidence), and also benefited from discussions with Jion Kim(Faculty of Forensic Information Science and Technology, Hallim University); Lanu Kim(School of Digital Humanities and Computational Social Sciences, KAIST). The authors thank the anonymous reviewers and journal editors. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. //learnprompting.org/docs/prompt_hacking/defensive_ measures/sandwich_defense, accessed on 05.10.2014. All authors reviewed the results and approved the final informedness, markedness arXiv:2010.16061 (2020). Data will be made available on request. factuality arXiv:2005.00661 (2020). Declaration of competing interest psychological measurement 20 (1960) 37–46. version of the manuscript. data, Biometrics 33 (1977) 159174. Acknowledgement Data-availability arXiv:2005.14165 (2020). abstractive Schulhoff, Sandwich defense, 2024. URL: S. D. Afchar, V. Nozick, J. Yamagishi, I. Echizen, Mesonet: a compact facial video forgery detection network, in: 2018 IEEE international workshop on information forensics and security (WIFS), IEEE, 2018, pp. 1–7. A. L. S. Orozco, C. Q. Huam´an, D. P. Alvarez, L. J. G. Villalba, A´ machine learning forensics technique to detect post-processing in digital videos, Future Generation Computer Systems 111 (2020) 199–212. H. Kim, D. Kim, J. Lee, C. Yoon, D. Choi, M. Gim, J. Kang, Lapis: language model-augmented police investigation system, in: Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 2024, pp. 4637–4644. P. H. Rughani, Artificial intelligence based digital forensics framework., International Journal of Advanced Research in Computer Science 8 (2017). E. Yang, Necessity of regulation on the development and use of generative ai -focusing on large language models conversational a.i. services (llms ai), SungKyunKwan Law Review 35 (2023) 293–325. M. A. Qamhan, H. Altaheri, A. H. Meftah, G. Muhammad, Y. A. Alotaibi, Digital audio forensics: microphone and environment classification using deep learning, Ieee Access 9 (2021) 62719– 62733. Y. Liu, Y. Yao, J.-F. Ton, X. Zhang, R. G. H. Cheng, Y. Klochkov, M. F. Taufiq, H. Li, Trustworthy llms: A survey and guideline for evaluating large language models’ alignment, arXiv preprint arXiv:2308.05374 (2023). D. M. Powers, Evaluation: from precision, recall and f-measure to roc, arXiv preprint and correlation, J. Maynez, S. Narayan, B. Bohnet, R. McDonald, On faithfulness and preprint summarization, arXiv in G. Landis JRKoch, The measurement of observer agreement for categorical J. Cohen, A coefficient of agreement for nominal scales, Educational and T. B. Brown, Language models are few-shot learners, arXiv preprint https: This preprint research paper has not been peer reviewed. shot learners, arXiv preprint arXiv:2012.15723 (2020). T. Gao, A. Fisch, D. Chen, Making pre-trained language models better few- Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, H. Wang, Retrieval- augmented generation for large language models: A survey, arXiv preprint arXiv:2312.10997 (2023). T. Schick, H. Schu¨tze, It’s not just size that matters: Small language models are also few-shot learners, arXiv preprint arXiv:2009.07118 (2020). Z. Zhao, E. Wallace, S. Feng, D. Klein, S. Singh, Calibrate before use: Improving few-shot performance of language models, in: International conference on machine learning, PMLR, 2021, pp. 12697–12706. Preprint not peer reviewed This preprint research paper has not been peer reviewed.